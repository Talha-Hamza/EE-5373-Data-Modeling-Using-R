#   f <- f_values[i]
#   rows <- nrow(int06.dat)
#   N <- floor((1 - f) * rows)
#   D_f <- c()
#
#   for (j in 1:k) {
#     perm <- sample(rows)
#     upper_bound <- floor(f * rows)
#     train <- int06.dat[perm[1:upper_bound], ] # change the data set here
#     test  <- int06.dat[perm[(upper_bound+1):rows], ] # change the data set here
#
#     # Change the model as appropriate
#     model <- lm(nperf~clock+
#                   transistors +voltage+featureSize+channel+
#                   FO4delay+L1icache+L2cache+sqrt(L2cache)+L3cache+sqrt(L3cache), data = train)
#
#     pred <- predict(model, newdata = test)
#     delta <- test$nperf - pred
#     D_f <- c(D_f, delta)
#   }
#
#   ci <- t.test(D_f, conf.level = 0.95)
#   mean_vals[i]  <- ci$estimate
#   lower_vals[i] <- ci$conf.int[1]
#   upper_vals[i] <- ci$conf.int[2]
# }
#
# # Plot mean ± CI vs. f for dataset chosen
# plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
#      xlab = "Training fraction (f)", ylab = "Mean prediction error",
#      main = "int06 benchmark")
# arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
# abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
#-------------------------------- fp95 data -----------------------------------
# fp95 model testing
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp95.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp95.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp95.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors +dieSize+voltage+
sqrt(L1icache)+L1dcache, data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "int06 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp95.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp95.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp95.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors +dieSize+voltage+
sqrt(L1icache)+L1dcache, data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "fp95 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp95.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp95.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp95.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors +dieSize+voltage+
sqrt(L1icache)+L1dcache, data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "fp95 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp95.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp95.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp95.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors +dieSize+voltage+
sqrt(L1icache)+L1dcache, data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "fp95 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp95.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp95.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp95.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors +dieSize+voltage+
sqrt(L1icache)+L1dcache, data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "fp95 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
#-------------------------------- fp95 data -----------------------------------
# fp95 model testing
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp95.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp95.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp95.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors +dieSize+voltage+
sqrt(L1icache)+L1dcache, data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "fp95 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
qt (1 -0.05 / 2 , df =23 -1)
Getting2NoU <- read.csv("https://raw.githubusercontent.com/Talha-Hamza/STAT3011-015-F25/refs/heads/main/Getting2NoU.csv",
header = TRUE)
exercise <- Getting2NoU$exercise
hist(exercise,
main = "Histogram of Daily Exercise (hr/day)",
xlab = "Exercise (hours per day)")
qqnorm(exercise, main = "Q-Q Plot of Exercise Data")
qqline(exercise, col = "red", lwd = 2)
mean(exercise)
sd(exercise)          # Sample standard deviation
length(exercise)      # Sample size
# Compute standard error manually
se <- sd(exercise) / sqrt(length(exercise))
se
sd(exercise)
mean(exercise) + c(-1, 1) * qt(1-0.05/2, length(exercise) - 1) * sd(exercise) / sqrt(length(exercise))
sample_mean <- mean(exercise)
sample_sd <- sd(exercise)          # Sample standard deviation
n <- length(exercise)      # Sample size
# Compute standard error manually
se <- sample_sd / sqrt(n))
# Compute standard error manually
se <- sample_sd / sqrt(n)
se
sample_mean + c(-1, 1) * qt(1-0.05/2, n - 1) * se
t.test(exercise, conf.level = 0.95, alternative = "two.sided")
# Optional: Check for outliers visually
boxplot(exercise, main = "Boxplot of Daily Exercise", ylab = "Hours per Day")
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp06.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp06.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp06.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors+featureSize+channel+
FO4delay+L1icache+L2cache+sqrt(L2cache)+L3cache+sqrt(L3cache), data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Read the data from the csv file.
processors <- read.csv(file.choose())
################################################################
#
# This function returns the data from the desired column.
# Example:  clock<-get_column("Fp2000","Processor.Clock..MHz.")
get_column <- function(x,y) {
# x = string with the name of the desired benchmark
# y = desired column
#
# Find the indices of all rows that have an entry for the
# indicated benchmark
benchmark <- paste(paste("Spec",x,sep=""),"..average.base.",
sep="")
ix <- !is.na(processors[,benchmark])
return(processors[ix,y])
}
################################################################
################################################################
# This function extracts the interesting data columns for the given benchmark
# program and returns a dataframe with these columns.
extract_data <- function(benchmark) {
temp <- paste(paste("Spec",benchmark,sep=""),"..average.base.", sep="")
# perf = the performance reported in the database
perf <- get_column(benchmark,temp)
#nperf = performance normalized to the overall range
max_perf <- max(perf)
min_perf <- min(perf)
range <- max_perf - min_perf
nperf <- 100 * (perf - min_perf) / range
clock <- get_column(benchmark,"Processor.Clock..MHz.")
threads <- get_column(benchmark,"Threads.core")
cores <- get_column(benchmark,"Cores")
TDP <- get_column(benchmark,"TDP")
transistors <- get_column(benchmark,"Transistors..millions.")
dieSize <- get_column(benchmark,"Die.size..mm.2.")
voltage <- get_column(benchmark,"Voltage..low.")
featureSize <- get_column(benchmark,"Feature.Size..microns.")
channel <- get_column(benchmark,"Channel.length..microns.")
FO4delay <- get_column(benchmark,"FO4.Delay..ps.")
L1icache <- get_column(benchmark,"L1..instruction...on.chip.")
L1dcache <- get_column(benchmark,"L1..data...on.chip.")
L2cache <- get_column(benchmark,"L2..on.chip.")
L3cache <- get_column(benchmark,"L3..on.chip.")
return(data.frame(nperf,perf,clock,threads,cores,TDP,transistors,dieSize,voltage,featureSize,channel,FO4delay,L1icache,L1dcache,L2cache,L3cache))
}
################################################################
# Extract a new data frame for each of the benchmark programs available in the data set.
int92.dat <- extract_data("Int1992")
fp92.dat <- extract_data("Fp1992")
int95.dat <- extract_data("Int1995")
fp95.dat <- extract_data("Fp1995")
int00.dat <- extract_data("Int2000")
fp00.dat <- extract_data("Fp2000")
int06.dat <- extract_data("Int2006")
fp06.dat <- extract_data("Fp2006")
#-------------------------------- fp06 data -----------------------------------
f_values <- seq(0.1, 0.9, by = 0.1)
mean_vals <- numeric(length(f_values))
lower_vals <- numeric(length(f_values))
upper_vals <- numeric(length(f_values))
k <- 100
for (i in seq_along(f_values)) {
f <- f_values[i]
rows <- nrow(fp06.dat)
N <- floor((1 - f) * rows)
D_f <- c()
for (j in 1:k) {
perm <- sample(rows)
upper_bound <- floor(f * rows)
train <- fp06.dat[perm[1:upper_bound], ] # change the data set here
test  <- fp06.dat[perm[(upper_bound+1):rows], ] # change the data set here
# Change the model as appropriate
model <- lm(nperf~clock+
transistors+featureSize+channel+
FO4delay+L1icache+L2cache+sqrt(L2cache)+L3cache+sqrt(L3cache), data = train)
pred <- predict(model, newdata = test)
delta <- test$nperf - pred
D_f <- c(D_f, delta)
}
ci <- t.test(D_f, conf.level = 0.95)
mean_vals[i]  <- ci$estimate
lower_vals[i] <- ci$conf.int[1]
upper_vals[i] <- ci$conf.int[2]
}
# Plot mean ± CI vs. f for dataset chosen
plot(f_values, mean_vals, ylim = range(c(lower_vals, upper_vals)),
xlab = "Training fraction (f)", ylab = "Mean prediction error",
main = "fp06 benchmark")
arrows(f_values, lower_vals, f_values, upper_vals, length = 0.05, angle = 90, code = 3)
abline(h = 0, col = "lightgreen", lty = 2, lwd = 2)
setwd("~/UMN-EE-coursework/Fall 2025/EE 5373/lab6")
library(dplyr)
raw_house_data <- read.csv("kc_house_data.csv")
house_data <- na.omit(raw_house_data)
# Define new function
price_prediction_error <- function(price, bedrooms, bathrooms, sqft_living,
sqft_lot, grade, yr_built) {
house_info <- data.frame(price, bedrooms, bathrooms, sqft_living, sqft_lot, grade, yr_built)
rows <- nrow(house_info)
f <- 0.6
perm <- house_info[sample(rows), ]
train.dat <- perm[1:floor(f * rows), ]
test.dat  <- perm[(floor(f * rows) + 1):rows, ]
# Linear model
house.lm <- lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + grade
+ yr_built, data = train.dat)
# Predictions & RMSE
pred <- predict(house.lm, newdata = test.dat)
rmse <- sqrt(mean((test.dat$price - pred)^2))
return(rmse)
}
# Group by zipcode
data_by_zipcode <- house_data %>%
group_by(zipcode) %>%
summarize(
count = n(),
med_price = median(price),
med_yr_built = median(yr_built),
error = price_prediction_error(price, bedrooms, bathrooms, sqft_living,
sqft_lot, grade, yr_built)
)
plot( x= data_by_zipcode$zipcode,
y= data_by_zipcode$error,
xlab = "Zipcodes",
ylab = "RMSE",
main = "Error vs Zipcode"
)
boxplot(data_by_zipcode$error,
ylab = "RMSE",
main = "Boxplot of Errors")
min(data_by_zipcode$error)
max(data_by_zipcode$error)
hist(data_by_zipcode$error,
breaks = 20,
main = "Distribution of RMSE Across Zip Codes",
xlab = "RMSE",
ylab = "Frequency")
cor(data_by_zipcode$med_price, data_by_zipcode$error)
plot(data_by_zipcode$med_price, data_by_zipcode$error,
pch = 19,
xlab = "Median Price",
ylab = "RMSE",
main = "Higher Priced Zip Codes = Higher RMSE?")
cor(data_by_zipcode$count, data_by_zipcode$error, use = "complete.obs")
plot(data_by_zipcode$count, data_by_zipcode$error,
pch = 19,
xlab = "Number of Houses in Zipcode",
ylab = "RMSE",
main = "Does Sample Size Affect RMSE?")
best_zipcodes <- data_by_zipcode %>%
arrange(error) %>%
head(10)
worst_zipcodes <- data_by_zipcode %>%
arrange(desc(error)) %>%
head(10)
best_zipcodes
worst_zipcodes
cor(data_by_zipcode$count, data_by_zipcode$error, use = "complete.obs")
# Calculate correlations with error
correlation_analysis <- data.frame(
variable = c("Sample_Size", "Price_Diversity", "Size_Diversity", "Age_Range", "Avg_Price", "Avg_Grade"),
correlation = c(
cor(data_by_zipcode$count, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$price_cv, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$sqft_living_sd, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$yr_built_range, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_price, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$avg_grade, data_by_zipcode$error, use = "complete.obs")
)
)
head(data_by_zipcode)
# Calculate correlations with error
correlation_analysis <- data.frame(
variable = c("Sample_Size", "Price_Diversity", "Size_Diversity", "Age_Range", "Avg_Price", "Avg_Grade"),
correlation = c(
cor(data_by_zipcode$count, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_price, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_yr_built, data_by_zipcode$error, use = "complete.obs"),
)
)
# Calculate correlations with error
correlation_analysis <- data.frame(
variable = c("Sample_Size", "Price_Diversity", "Size_Diversity", "Age_Range", "Avg_Price", "Avg_Grade"),
correlation = c(
cor(data_by_zipcode$count, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_price, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_yr_built, data_by_zipcode$error, use = "complete.obs")
)
)
print("Correlations with Prediction Error:")
print(correlation_analysis[order(abs(correlation_analysis$correlation), decreasing = TRUE), ])
# Identify the best and worst performing zip codes
top_10_worst <- data_by_zipcode[order(data_by_zipcode$error, decreasing = TRUE), ][1:10, ]
top_10_best <- data_by_zipcode[order(data_by_zipcode$error, decreasing = FALSE), ][1:10, ]
cat("\nTop 10 Worst Performing Zip Codes (Highest Error):\n")
print(top_10_worst[, c("zipcode", "error", "count", "price_cv", "med_price")])
correlation_analysis <- data.frame(
variable = c("Sample_Size", "Price_Diversity", "Size_Diversity", "Age_Range", "Avg_Price", "Avg_Grade"),
correlation = c(
cor(data_by_zipcode$count, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_price, data_by_zipcode$error, use = "complete.obs"),
cor(data_by_zipcode$med_yr_built, data_by_zipcode$error, use = "complete.obs")
)
)
print("Correlations with Prediction Error:")
print(correlation_analysis[order(abs(correlation_analysis$correlation), decreasing = TRUE), ])
# Identify the best and worst performing zip codes
top_10_worst <- data_by_zipcode[order(data_by_zipcode$error, decreasing = TRUE), ][1:10, ]
top_10_best <- data_by_zipcode[order(data_by_zipcode$error, decreasing = FALSE), ][1:10, ]
cat("\nTop 10 Worst Performing Zip Codes (Highest Error):\n")
print(top_10_worst[, c("zipcode", "error", "count", "med_yr_built", "med_price")])
cat("\nTop 10 Best Performing Zip Codes (Lowest Error):\n")
print(top_10_best[, c("zipcode", "error", "count", "med_yr_built", "med_price")])
